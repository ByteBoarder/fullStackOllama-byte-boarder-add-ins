apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ollama
  namespace: ollama-system
spec:
  interval: 5m
  chart:
    spec:
      chart: ollama
      version: 1.15.0
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system
  values:
    replicaCount: 1
    
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
        nvidia.com/gpu: "1"
      limits:
        cpu: "4"
        memory: "8Gi"
        nvidia.com/gpu: "1"
        
    nodeSelector:
      nvidia.com/gpu: "true"
      
    tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
      
    service:
      type: ClusterIP
      port: 11434
      
    ingress:
      enabled: false  # We can enable this later if needed

    ollama:
      gpu:
        enabled: true
      models:
        pull:
        # Pull these models at startup
        - llama2
        - mistral
        - codellama
